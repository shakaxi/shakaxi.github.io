#+title: Mathematics

As the titile indicates, this post is written to summarize the basic of mathematics.

* Special functions
- Gamma function
  \begin{align*}
    \Gamma(x) \triangleq \int_0^{+\infty} e^{-t} t^{x - 1} dt
  \end{align*}
  Particularly, $\forall x \in \mathbb{Z}^+$, $\Gamma(x) = (x-1)!$.
- Bessel functions
  + First kind
    \begin{align*}
      J_n(x) \triangleq \frac{1}{2\pi}\int_{-\pi}^{\pi} e^{j(x\sin\theta-n\theta)}d\theta
    \end{align*}
    Particularly, $\forall \phi \in [0, 2\pi)$, $J_0(x) = \dfrac{1}{2\pi} \int_\phi^{2\pi + \phi}e^{\pm jx\cos\theta}d\theta = \dfrac{1}{2\pi} \int_\phi^{2\pi + \phi}e^{\pm jx\sin\theta}d\theta$.
  + Second kind
- Q-function
  \begin{align*}
    Q(x) &\triangleq \frac{1}{\sqrt{2\pi}}\int_x^{+\infty}e^{-\frac{t^2}{2}} dt \\
    &= 1 - Q(-x)
  \end{align*}
- Error function and complementary error function
  \begin{align*}
    erf(x) &= \frac{2}{\sqrt{\pi}}\int_0^x e^{-t^2} dt \\
    &= 2Q(\sqrt{2}x) \\
    erfc(x)&= \frac{2}{\sqrt{\pi}}\int_x^{+\infty} e^{-t^2} dt \\
    &= 1 - erf(x) \\
    &= 1 - 2Q(\sqrt{2}x)
  \end{align*}
- Euler
  \begin{align*}
    e^{j\theta} &= \cos\theta + j\sin\theta \\
    \cos\theta &= \frac{e^{j\theta} + e^{-j\theta}}{2} \\
    \sin\theta &= \frac{e^{j\theta} - e^{-j\theta}}{2j}
  \end{align*}
* Matrix
** Vectorization
$\forall \mathbf{A} \in \mathbb{C}^{p \times n}$, $\mathbf{B} \in \mathbb{C}^{n \times m}$, $\mathbf{C} \in \mathbb{C}^{m \times q}$,
\begin{align}
  \text{vec}(\mathbf{ABC}) = (\mathbf{C}^T \otimes \mathbf{A}) \text{vec}(\mathbf{B}). \label{eq:vect}
\end{align}
Particularly,
- By setting $\mathbf{A} = \mathbf{I}_n$ and $\mathbf{C} = \mathbf{I}_m$ respectively, \eqref{eq:vect} becomes
\begin{align*}
  \text{vec}(\mathbf{BC}) &= (\mathbf{C}^T \otimes \mathbf{I}_n) \text{vec}(\mathbf{B}), \\
  \text{vec}(\mathbf{AB}) &= (\mathbf{I}_m \otimes \mathbf{A}) \text{vec}(\mathbf{B}).
\end{align*}
- If $\mathbf{B} = \text{diag}(\mathbf{b})$ is a diagonal matrix ($m = n$), $\mathbf{b} \in \mathbb{C}^n$, \eqref{eq:vect} becomes
  \begin{align*}
    \text{vec}(\mathbf{ABC}) = (\mathbf{C}^T \odot \mathbf{A})\mathbf{b},
  \end{align*}
  where $\odot$ is the [[*Khatri-Rao product][Khatri-Rao product]] operator.
** Khatri-Rao product
$\forall \mathbf{A} = \begin{bmatrix} \mathbf{a}_1 & \mathbf{a}_2 & \cdots &\mathbf{a}_k \end{bmatrix} \in \mathbb{C}^{n \times k}$ and $\mathbf{B} = \begin{bmatrix} \mathbf{b}_1 & \mathbf{b}_2 & \cdots & \mathbf{b}_k \end{bmatrix} \in \mathbb{C}^{m \times k}$, their Khatri-Rao product can be denoted by
\begin{align*}
  \mathbf{A} \odot \mathbf{B} = \begin{bmatrix}
      \mathbf{a}_1 \otimes \mathbf{b}_1 & \mathbf{a}_2 \otimes \mathbf{b}_2 & \cdots & \mathbf{a}_k \otimes \mathbf{b}_k
\end{bmatrix} \in \mathbb{C}^{mn \times k},
\end{align*}
where $\mathbf{a}_i \otimes \mathbf{b}_i = \text{vec}\left(\mathbf{b}_i \mathbf{a}_i^T\right) \in \mathbb{C}^{nm}$, $\mathbf{a}_i \in \mathbb{C}^n$, $\mathbf{b}_i \in \mathbb{C}^m$, $i = 1, 2, \ldots, k$.
* Calculus
** Differentiation
** Integration
- Newton-Leibniz
  \begin{align*}
    \int_a^b f(x) dx = F \mid_a^b = F(b) - F(a)
  \end{align*}
- Symmetry
  \begin{align*}
    \int_a^b f(x) dx &= \int_a^{\cfrac{a+b}{2}} \left[f(a+b-x) + f(x)\right] dx \\
                  &= \int_{\cfrac{a+b}{2}}^b \left[f(a+b-x) + f(x)\right] dx
  \end{align*}
  Particularly when $a = -b$, we have
  \begin{align*}
    \int_{-b}^b f(x) dx &= \int_0^b \left[ f(x) + f(-x) \right] dx \\
                     &= \begin{cases}
                       0, & f(x) = -f(-x); \\
                       2\int_0^b f(x) dx, & f(x) = f(-x).
                     \end{cases}
  \end{align*}
- Transformation
  + Given $\begin{cases} x = x(u, v) \\ y = y(u, v) \end{cases}$, if $x(u,v)$ and $y(u,v)$ are contiguous and differentiable, and Jacobian determinant $J = \left| \cfrac{D(x,y)}{D(u,v)} \right| = \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{vmatrix} \neq 0$, then we have
    \begin{align*}
      \iint_{D_{xy}} f(x,y)dxdy = \iint_{D_{uv}} f \left( x(u,v), y(u,v) \right) |J| dudv,
    \end{align*}
    where $D_{uv}$ is an one-to-one mapping of $D_{xy}$.

    Particularly, for the transformation from Cartesian to polar coordinate systems, $\begin{cases} x = r\cos\theta \\ y = r\sin\theta \end{cases}$, we have $J = r$ and
    \begin{align*}
      \iint_{D_{xy}} f(x,y)dxdy = \iint_{D_{r\theta}} f(r\cos\theta, r\sin\theta) rdrd\theta.
    \end{align*}
  + Given $\begin{cases} x = x(u, v, w) \\ y = y(u, v, w) \\ z = z(u, v, w) \end{cases}$, if $x(u,v,w)$, $y(u,v,w)$, and $z(u,v,w)$ are contiguous and differentiable, and Jacobian determinant $J = \left| \cfrac{D(x,y,z)}{D(u,v,w)} \right| = \begin{vmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} & \frac{\partial x}{\partial w} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} & \frac{\partial y}{\partial w} \\ \frac{\partial z}{\partial u} & \frac{\partial z}{\partial v} & \frac{\partial z}{\partial w}\end{vmatrix} \neq 0$, then we have
    \begin{align*}
      \iiint_{V_{xyz}} f(x,y,z)dxdydz = \iiint_{V_{uvw}} f \left( x(u,v,w), y(u,v,w), z(u,v,w) \right) |J| dudvdw,
    \end{align*}
    where $V_{uvw}$ is an one-to-one mapping of $V_{xyz}$.

    Particularly, for the transformation from Cartesian to cylindrical coordinate systems, $\begin{cases} x = r\cos\theta \\ y = r\sin\theta \end{cases}$, we have $J = r$ and
    \begin{align*}
      \iiint_{V_{xyz}} f(x,y,z)dxdydz = \iiint_{V_{r\theta z}} f(r\cos\theta, r\sin\theta, z) rdrd\theta dz.
    \end{align*}

    Particularly, for the transformation from Cartesian to spherical coordinate systems, $\begin{cases} x = r\sin\theta\cos\phi \\ y = r\sin\theta\sin\phi \\ z = r\cos\theta\end{cases}$, we have $J = r^2 \sin\theta$ and
    \begin{align*}
      \iiint_{V_{xyz}} f(x,y,z)dxdydz = \iiint_{V_{r\theta\phi}} f(r\sin\theta\cos\phi, r\sin\theta\sin\phi, r\cos\theta) r^2\sin\theta drd\theta d\phi.
    \end{align*}
** Maximum and minimum values
For a differentiable function $f: \mathbb{R}^{n} \to \mathbb{R}$, zero gradient is the necessary condition of an extreme point.
- If Hessian matrix at an extreme point $\mathbf{x}_0$ is positive definite, i.e., $\nabla f(\mathbf{x}_0) = \mathbf{0}$, $\mathbf{H}(\mathbf{x}_0) \succ 0$, the extreme point corresponds with a local minimum.
- If Hessian matrix at an extreme point $\mathbf{x}_0$ is negative definite, i.e., $\nabla f(\mathbf{x}_0) = \mathbf{0}$, $\mathbf{H}(\mathbf{x}_0) \prec 0$, the extreme point corresponds with a local maximum.
* Transform
** Fourier
** Laplace
* Series
** Taylor
- For function $f(x)$, if $f^{(n)}(x_0)$ exists, its Taylor expansion can be written as
  \begin{align*}
    f(x) = f(x_0) + f^{\prime}(x_0)(x - x_0) + \cdots + \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n + o\left( (x - x_0)^n\right).
  \end{align*}
- When $n \to \infty$, Taylor expansion above becomes power series.
  \begin{align*}
    f(x) = f(x_0) + f^{\prime}(x_0)(x - x_0) + \cdots + \frac{f^{(n)}(x_0)}{n!}(x - x_0)^n + \cdots.
  \end{align*}
  The power series in the right side is termed Taylor series. Particularly, if $x_0 = 0$, the series is termed Maclaurin series, i.e.,
  \begin{align*}
    f(x) = f(0) + f^{\prime}(0)x + \cdots + \frac{f^{(n)}(0)}{n!}x^n + \cdots.
  \end{align*}
- Examples
  \begin{align*}
    e^x &= \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2} + \cdots; \quad \mathrm{ROC}: (-\infty, +\infty) \\
    \sin x &= \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{(2n+1)!} = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots; \quad \mathrm{ROC}: (-\infty, +\infty) \\
    \cos x &= \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{(2n)!} = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots; \quad \mathrm{ROC}: (-\infty, +\infty) \\
    (1+x)^a &= \sum_{n=0}^{\infty} C_a^nx^n = 1 + ax + \frac{a(a-1)}{2}x^2 + \cdots; \quad \mathrm{ROC}: (-1, 1) \\
    \ln(1+x) &= \sum_{n=0}^{\infty} \frac{(-1)^n x^{n+1}}{n+1} = x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots; \quad \mathrm{ROC}: (-1, 1] \\
    \arctan x &= \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n+1}}{2n+1} = x - \frac{x^3}{3} + \frac{x^5}{5} - \cdots; \quad \mathrm{ROC}: [-1, 1]
  \end{align*}
