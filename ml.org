#+TITLE: Machine Learning

* Introducation
Machine learning (ML) is a subfield of artificial intelligence (AI).
* Application
** Image processing
- Image tagging
- Optical character recognition (OCR)
- Self-driving cars
** Text analysis
- Spam filtering
- Sentiment analysis
- Information extract
** Data mining
Discover patterns and make predictions from data.
- Anomaly detection
- Association rules
- Grouping
- Predictions
** Video games & robotics
* Algorithms
- ML algorithms try to solve the same problem: learn to transform every input to the correct output.
- ML algorithms try to make generalizations, and create the simplest hypothesis (with fewest assumptions) that explains most of the training examples.
** Categories
*** Supervised learning
For each training example, the correct output must be provided.
*** Unsupervised learning
The training examples but not the desired output are the input.
** Algorithms
*** Support vector machines (SVM's)
The model tries to build a set of hyperplanes in a high dimensional space which tries to separate instances of different classes by getting the largest separation between the nearest instances from different classes.
*** Probabilistic models
- The models try to predict the correct response by modeling the problem with a probability distribution, e.g., Naive Bayes classifiers.
- It returns the prediction as well as the corresponding degree of certainty.
*** Deep learning
- It is based on the artificial neural network models. They try to emulate (in a very simplified way) the way the brain works.
- The network consist of a huge set of interconnected neurons, i.e., the basic processing unit, organized in various layers.
** Feature engineering
*** Feature extraction
Before fed to the ML algorithms, the raw data should be tranformed into a vector of features.
*** Feature selection
- Score each feature and return only the most valuable ones.
- Do not use huge feature sets.
** Performance metric
- The instances for training and testing must be disjoint.
* Deep learning
A subfield of machine learning.
** Recurrent neural network (RNN)
** Convolutional neural network (CNN)
- Take a 2-D array as input.
- Direct operation on the image rather than feature extraction
- Basic ideas
  + Local receptive field
  + Convolution
  + Pooling
* Neural network
- The network is composed of an input layer, an output layer, and optionally a series of hidden layers.
- Input layer does not carry out any operation/function, but the hidden and output layers consist of functional neurons, i.e., have activation functions.
** McCulloch and Pitts (M-P) model for neurons
\begin{align*}
  y = f(\mathbf{w} \mathbf{x} - \theta)
\end{align*}
where \(f\) is termed /activation function/, which can be
\begin{align*}
  \text{sign}(x) = \begin{cases}
    1, & x \ge 0; \\
    0, & x < 0.
  \end{cases}
\end{align*}
or
\begin{align*}
  \text{sigmoid}(x) = \frac{1}{1 + e^{-x}}.
\end{align*}
** Multi-layer feedforward neural network
- Full connection between neurons of the adjacent ranks.
- No connection between neurons of the identical rank.
** Cross-entropy
\begin{align*}
  \mathcal{H}_{p}(p^{\prime}) = \sum_{i}p_{i}\log\frac{1}{p^{\prime}_i}
\end{align*}
where
- \(p\) is the true probability distribution.
- \(p^\prime\) is the predicted probability distribution.
** Softmax
Softmax function can transform scores/logits into a valid probability distribution.
\begin{align*}
  \text{softmax}(\mathbf{x}_{i}) = \frac{\exp(\mathbf{x}_{i})}{\sum_{j}\exp(\mathbf{x}_{j})}
\end{align*}
