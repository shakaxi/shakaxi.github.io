#+title: Python - Pytorch

* 张量
在pytorch中，神经网络的输入、输出、以及网络的参数等都采用了“张量”的数据结构。张量与numpy中的多维数组(=numpy.ndarray=)非常类似，区别在于张量可以在GPU或其他硬件上运行。
* 神经网络
** 正向传播与反向传播
神经网络的训练包括两个步骤
- 正向传播 :: 神经网络根据输入对输出进行最佳预测。
- 反向传播 :: 神经网络根据预测误差调整其参数。

当调用误差张量的成员函数 =.backward()= 时，开始反向传播，整个网络将被微分。Pytorch的自动差分引擎 =torch.autograd= 会对计算每个属性 =requires_grad=True= 的网络参数的梯度并将其累积在该网络参数的 =.grad= 属性中。所以，在反向传播开始之前，需要清空神经网络各个参数和反向传播的梯度缓冲区。

然后，需要从 =torch.optim= 中加载一个优化器(如随机梯度下降，SGD)。通过调用该优化器的 =.step()= 方法启动梯度下降，优化器会根据每个网络参数的梯度(存储于其 =.grad= 属性中)来对其进行调整。
** 典型训练过程
- 构建含有一些可学习参数(或权重)的神经网络。
- 遍历训练数据集。
- 让输入通过网络得到输出。
- 计算预测误差(或损失函数)。
- 通过反向传播，计算并存储网络各个参数的梯度。
- 根据各个参数的梯度对其进行调整，如 =weight = weight - learning_rate x gradient= 。
** 定义网络
可以使用 =torch.nn= 包来构建神经网络。
- 基类 =torch.nn.Module= 包含
  + 网络的各个层(作为类属性)，如输入层、输出层、隐藏层等。
  + 成员函数 =.forward= 。
  + 成员函数 =.zero_grad= ，用于清空神经网络中所有参数的梯度缓冲区。
- 构建神经网络可以通过定义以 =torch.nn.Module= 为基类的派生类完成。
- 只需要在派生类中重载成员函数 =.forward= ， =torch.autograd= 就会自动为网络各个参数定义 =.backward()= 函数。
- 所构建的神经网络的可学习参数由所述派生类的成员函数 =.parameters()= 返回。
- =torch.nn= 包中定义了很多损失函数(如 =torch.nn.MSELoss=)。
* 数据集
自定义的数据集可以通过定义以(=torch.utils.data.Dataset=)为基类的派生类完成。具体地，需要重载如下三个函数：
- =__init__(self)=: 载入数据；
- =__getitem__(self, index)=: 根据索引返回数据集中相应的样本点；
- =__len(self)__=: 返回数据集的大小，也即数据集中样本点的数量。
* 数据加载器
基于自定义的数据集，可以使用类 =torch.utils.data.DataLoader= 来生成相应的数据加载器。如
#+begin_src python
  from torch.utils.data import Dataset, DataLoader
  dataloader = DataLoader(dataset=My_Dataset, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=NUM_WORKERS)
#+end_src
其中，
- =My_Dataset= 为自定义的数据集；
- =BATCH_SIZE= 为每个batch中含有的样本点个数；
- =SHUFFLE= 为布尔型变量，用于指定数据集是否会被随机打乱顺序；
- =NUM_WORKERS= 为用于数据加载的线程个数。
