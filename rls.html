<!DOCTYPE html>
<html lang="en">
<head>
<!-- 2023-01-28 Sat 01:49 -->
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Least Square and Its Iterative Approach (RLS)</title>
<meta name="author" content="shaka" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="default.css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Least Square and Its Iterative Approach (RLS)</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#orgdc1af79">Introduction</a></li>
<li><a href="#sec:ls">Least Square (LS)</a></li>
<li><a href="#sec:rls">Recursive Least Square (RLS)</a>
<ul>
<li><a href="#org8ac09a9">Iteration of Auto/Cross-Correlation</a></li>
<li><a href="#orgf727bc4">Iteration of LS Filter</a></li>
<li><a href="#orgc1db719">Procedure</a></li>
</ul>
</li>
</ul>
</div>
</div>

<div id="outline-container-orgdc1af79" class="outline-2">
<h2 id="orgdc1af79">Introduction</h2>
<div class="outline-text-2" id="text-orgdc1af79">
<p>
As is well-known, <i>least square (LS)</i> is the optimal linear unbiased estimator, which plays an extremely important role in the classic estimation theory. This document motivates to illustrate the fundamental of LS algorithm. However, LS algorithm suffers rather high complexity, especially for the operation of matrix inversion, which greatly limits its application in practice. In order to reduce the computational complexity, an iterative approach, a.k.a. <i>recursive LS (RLS)</i>, has been proposed.
</p>

<p>
The remainder part of the document is structured as follows. In Section <a href="#sec:ls">Least Square (LS)</a>, the principle of LS algorithm is derived in detail. Then its iterative solution, i.e., RLS algorithm, follows in Section <a href="#sec:rls">Recursive Least Square (RLS)</a>.
</p>
</div>
</div>

<div id="outline-container-sec:ls" class="outline-2">
<h2 id="sec:ls">Least Square (LS)</h2>
<div class="outline-text-2" id="text-sec:ls">
<p>
A series of signal \(x(n)\) passes through a linear dynamic channel, characterized by <i>channel impulse response (CIR)</i> \(\mathbf{h}(n) = [h(n, 0), h(n, 1), \ldots, h(n, p-1)]^T\), where \(p\) is the memory length of the channel. Then the output signal \(y(n)\) can be expressed by
</p>
\begin{align*}
  y(n) = \sum_{\tau=0}^{p-1} h(n, \tau)x(n-\tau) + v(n)
\end{align*}
<p>
where \(v(n)\) is additive Gaussian noise, \(n = 0, 1, \ldots\).
</p>

<p>
In order to recover the desired signal \(x(n)\), a dynamic <i>finite impulse response (FIR)</i> filter \(\mathbf{w}(n) = [w(n, 0), w(n, 1), \ldots, w(n, q-1)]^T\) takes \(\mathbf{y}(n) = [y(n), y(n-1), \ldots, y(n-q+1)]^T\) as input and outputs \(\tilde{x}(n)\).
</p>
\begin{align*}
  \tilde{x}(n) &= \sum_{\tau=0}^{q-1} w(n, \tau) y(n-\tau) \\
  &= \mathbf{w}^T(n) \mathbf{y}(n).
\end{align*}

<p>
Given \(\mathbf{w}(n)\), the sample-wise error<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup> can be denoted as
</p>
\begin{align*}
  e_{\mathbf{w}(n)}(i) &= \tilde{x}(i) - x(i) \\
  &= \mathbf{w}^T(n) \mathbf{y}(i) - x(i), \quad i = 0, 1, \ldots, n.
\end{align*}
<p>
Taking \(C_{\mathbf{w}(n)} = \sum_{i=0}^n \lambda^{n-i} e_{\mathbf{w}(n)}^2(i)\) as cost function<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup>, the filter can be obtained by making
</p>
\begin{align*}
  \frac{\partial C_{\mathbf{w}(n)}}{\partial \mathbf{w}(n)} &= 2\sum_{i=0}^n \lambda^{n-i} e_{\mathbf{w}(n)}(i) \frac{\partial e_{\mathbf{w}(n)}(i)}{\partial \mathbf{w}(n)} \\
                                                            &= 2\sum_{i=0}^n \lambda^{n-i} \mathbf{y}(i)[\mathbf{y}^T(i) \mathbf{w}(n) - x(i)]  \\
  &= 2 \left[ \sum_{i=0}^n \lambda^{n-i} \mathbf{y}(i)\mathbf{y}^T(i) \right] \mathbf{w}(n) - 2 \sum_{i=0}^{n} \lambda^{n-i}x(i) \mathbf{y}(i) \\
  &= \mathbf{0}.
\end{align*}
<p>
Clearly, the impulse response of the filter can be written as
</p>
\begin{align}
  \label{eq:ls}
\mathbf{w}(n) = \left[ \sum_{i=0}^n \lambda^{n-i} \mathbf{y}(i)\mathbf{y}^T(i) \right]^{-1} \sum_{i=0}^{n} \lambda^{n-i}x(i) \mathbf{y}(i)
\end{align}
<p>
For simple description, define <i>weighted auto-correlation</i> matrix as
</p>
\begin{align}
  \label{eq:cov}
\mathbf{R}(n) \triangleq \sum_{i=0}^n \lambda^{n-i} \mathbf{y}(i)\mathbf{y}^T(i)
\end{align}
<p>
and <i>weighted cross-correlation</i> vector as
</p>
\begin{align}
  \label{eq:corr}
\mathbf{r}(n) \triangleq \sum_{i=0}^{n} \lambda^{n-i}x(i) \mathbf{y}(i).
\end{align}
<p>
Then, by substituting \eqref{eq:cov} and \eqref{eq:corr} into \eqref{eq:ls}, the impulse response can be rewritten as
</p>
\begin{align}
  \label{eq:ls-brief}
\mathbf{w}(n) = \mathbf{R}^{-1}(n) \mathbf{r}(n).
\end{align}
</div>
</div>

<div id="outline-container-sec:rls" class="outline-2">
<h2 id="sec:rls">Recursive Least Square (RLS)</h2>
<div class="outline-text-2" id="text-sec:rls">
<p>
In this section, RLS algorithm is derived in detail.
</p>
</div>
<div id="outline-container-org8ac09a9" class="outline-3">
<h3 id="org8ac09a9">Iteration of Auto/Cross-Correlation</h3>
<div class="outline-text-3" id="text-org8ac09a9">
<p>
According to \eqref{eq:cov}, we have
</p>
\begin{align}
  \mathbf{R}(n) &= \lambda \sum_{i=0}^{n-1} \lambda^{n-1-i} \mathbf{y}(i)\mathbf{y}^T(i) + \mathbf{y}(n) \mathbf{y}^T(n) \nonumber \\
  &= \lambda \mathbf{R}(n-1) + \mathbf{y}(n) \mathbf{y}^T(n). \label{eq:iter-R}
\end{align}
<p>
Applying matrix inversion lemma to \eqref{eq:iter-R} can yield the iteration between \(\mathbf{R}^{-1}(n)\) and \(\mathbf{R}^{-1}(n-1)\), i.e.,
</p>
\begin{align*}
\mathbf{R}^{-1}(n) = \frac{1}{\lambda} \left[ \mathbf{R}^{-1}(n-1) - \frac{\mathbf{R}^{-1}(n-1) \mathbf{y}(n) \mathbf{y}^T(n) \mathbf{R}^{-1}(n-1)}{\lambda + \mathbf{y}^T(n) \mathbf{R}^{-1}(n-1) \mathbf{y}(n)} \right]
\end{align*}
<p>
For the sake of compact expression, denote \(\mathbf{P}(n) = \mathbf{R}^{-1}(n)\) and define <i>Kalman gain</i> vector
</p>
\begin{align}
  \mathbf{k}(n) &= \frac{\mathbf{R}^{-1}(n-1) \mathbf{y}(n)}{\lambda + \mathbf{y}^T(n) \mathbf{R}^{-1}(n-1) \mathbf{y}(n)} \nonumber \\
  &= \frac{\mathbf{P}(n-1) \mathbf{y}(n)}{\lambda + \mathbf{y}^T(n) \mathbf{P}(n-1) \mathbf{y}(n)}. \label{eq:kalman-gain}
\end{align}
<p>
Then, the iteration above becomes
</p>
\begin{align}
    \label{eq:iter-p}
  \mathbf{P}(n) = \frac{1}{\lambda} [\mathbf{P}(n-1) - \mathbf{k}(n) \mathbf{y}^T(n) \mathbf{P}(n-1)].
\end{align}
<p>
By reducing the entry of \(\mathbf{k}(n) \mathbf{y}^T(n) \mathbf{P}(n-1) \mathbf{y}(n)\) in \eqref{eq:kalman-gain} and \eqref{eq:iter-p}, we can get
</p>
\begin{align}
  \label{eq:k-p}
\mathbf{k}(n) = \mathbf{P}(n) \mathbf{y}(n).
\end{align}

<p>
Simlarly, according to \eqref{eq:corr}, the iteration of the weighted cross-correlation is available, i.e.,
</p>
\begin{align}
  \mathbf{r}(n) &= \lambda \sum_{i=0}^{n-1} \lambda^{n-1-i}x(i) \mathbf{y}(i) + x(n) \mathbf{y}(n) \nonumber \\
  &= \lambda \mathbf{r}(n-1) + x(n) \mathbf{y}(n). \label{eq:iter-r}
\end{align}
</div>
</div>

<div id="outline-container-orgf727bc4" class="outline-3">
<h3 id="orgf727bc4">Iteration of LS Filter</h3>
<div class="outline-text-3" id="text-orgf727bc4">
<p>
Substituting \eqref{eq:iter-p}, \eqref{eq:k-p} and \eqref{eq:iter-r} into \eqref{eq:ls-brief} produces the iteration between \(\mathbf{w}(n)\) and \(\mathbf{w}(n-1)\), i.e.,
</p>
\begin{align}
  \label{eq:iter-w}
  \mathbf{w}(n) &= \mathbf{P}(n) \mathbf{r}(n) \nonumber \\
                &= \mathbf{P}(n) [ \lambda \mathbf{r}(n-1) + x(n) \mathbf{y}(n) ] \nonumber \\
                & = [\mathbf{P}(n-1) - \mathbf{k}(n) \mathbf{y}^T(n) \mathbf{P}(n-1)] \mathbf{r}(n-1) + x(n) \mathbf{k}(n) \nonumber \\
                & = \mathbf{w}(n-1) - \mathbf{k}(n) \mathbf{y}^T(n) \mathbf{w}(n-1) + x(n) \mathbf{k}(n) \nonumber \\
  &= \mathbf{w}(n-1) + \alpha(n) \mathbf{k}(n)
\end{align}
<p>
where
</p>
\begin{align}
  \label{eq:apriori-err}
\alpha(n) \triangleq x(n) - \mathbf{y}^T(n) \mathbf{w}(n-1)
\end{align}
<p>
is termed <i>a priori estimation error</i>.
</p>
</div>
</div>

<div id="outline-container-orgc1db719" class="outline-3">
<h3 id="orgc1db719">Procedure</h3>
<div class="outline-text-3" id="text-orgc1db719">
<p>
By collecting the operations scattered above, the detailed procedure of RLS algorithm can be summarized as follows, where the values of the two parameters should be offered in advance. \(\lambda \in (0, 1]\) is the forgetting factor, while \(\delta\) is a small positive number used for \(\mathbf{P}(0)\) initialization.
</p>

<table align="center">


<colgroup>
<col  class="org-left">

<col  class="org-left">
</colgroup>
<tbody>
<tr>
<td class="org-left"><b>Init.</b>:</td>
<td class="org-left">\(\mathbf{y}(0) = \mathbf{0}\), \(\mathbf{w}(0) = \mathbf{0}\), \(\mathbf{P}(0) = \delta \mathbf{I}\).</td>
</tr>

<tr>
<td class="org-left"><b>for</b></td>
<td class="org-left">\(n = 1, 2, \ldots\) <b>do</b></td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">\(\mathbf{y}(n) = [y(n), y(n-1), \ldots, y(n-q+1)]^T\)</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">\(\mathbf{k}(n) = \dfrac{\mathbf{P}(n-1) \mathbf{y}(n)}{\lambda + \mathbf{y}^T(n) \mathbf{P}(n-1) \mathbf{y}(n)}\)</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">\(\mathbf{P}(n) = \dfrac{1}{\lambda} [\mathbf{P}(n-1) - \mathbf{k}(n) \mathbf{y}^T(n) \mathbf{P}(n-1)]\)</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">\(\alpha(n) \triangleq x(n) - \mathbf{y}^T(n) \mathbf{w}(n-1)\)</td>
</tr>

<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">\(\mathbf{w}(n) = \mathbf{w}(n-1) +\alpha(n)\mathbf{k}(n)\)</td>
</tr>

<tr>
<td class="org-left"><b>end</b></td>
<td class="org-left"><b>for</b></td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
To be precise, it is <i>a posterior estimation error</i>.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Herein, \(\lambda \in (0, 1]\) is termed <i>forgetting factor</i>.
</p></div></div>


</div>
</div></div>
</body>
</html>